{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在clean1中提取选定日期内所有股票数据并，并与adjustment数据merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, String, create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.declarative import declarative_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(path):\n",
    "    path_list = []\n",
    "    for i in os.listdir(path):\n",
    "        path_list.append(i)\n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "# engine=create_engine(\"mysql+pymysql://research:research@10.10.10.118:3306/china_stocks?charset=utf8\", echo=True)\n",
    "engine=create_engine(\"mysql+pymysql://research:research@10.10.10.118:3306/china_stocks?charset=utf8\", echo=False)\n",
    "DBSession = sessionmaker(bind=engine)  \n",
    "session = DBSession()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_sql(\"select * from session_end_ticks where unique_symbol like '000001.SZE.STK' and date_time >= '2018-01-01' and date_time <= '2018-12-31' \", con = engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/data/share/china_stocks/data/index_weights/000905'\n",
    "time_line = func(PATH)\n",
    "time_line.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(time_line)):\n",
    "    if int(time_line[i][-12:-8])>2009:\n",
    "        time_start_index = i\n",
    "        break        \n",
    "for i in range(len(time_line)):\n",
    "    if int(time_line[i][-12:-8])>2017:\n",
    "        time_end_index = i\n",
    "        break\n",
    "time_line = time_line[time_start_index:time_end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 20100129.csv 500\n",
      "1 20100204.csv 499\n",
      "2 20100226.csv 500\n",
      "3 20100304.csv 500\n",
      "4 20100331.csv 500\n",
      "5 20100430.csv 500\n",
      "6 20100531.csv 500\n",
      "7 20100630.csv 500\n",
      "8 20100730.csv 500\n",
      "9 20100831.csv 500\n",
      "10 20100930.csv 500\n",
      "11 20101029.csv 500\n",
      "12 20101130.csv 500\n",
      "13 20101231.csv 500\n",
      "14 20110131.csv 500\n",
      "15 20110228.csv 500\n",
      "16 20110328.csv 500\n",
      "17 20110330.csv 500\n",
      "18 20110331.csv 500\n",
      "19 20110406.csv 500\n",
      "20 20110429.csv 500\n",
      "21 20110531.csv 500\n",
      "22 20110630.csv 500\n",
      "23 20110729.csv 500\n",
      "24 20110831.csv 500\n",
      "25 20110930.csv 500\n",
      "26 20111031.csv 500\n",
      "27 20111103.csv 500\n",
      "28 20111130.csv 500\n",
      "29 20111230.csv 500\n",
      "30 20120131.csv 500\n",
      "31 20120229.csv 500\n",
      "32 20120330.csv 500\n",
      "33 20120427.csv 500\n",
      "34 20120531.csv 500\n",
      "35 20120629.csv 500\n",
      "36 20120731.csv 500\n",
      "37 20120831.csv 500\n",
      "38 20120903.csv 500\n",
      "39 20120928.csv 500\n",
      "40 20121031.csv 500\n",
      "41 20121130.csv 500\n",
      "42 20121231.csv 500\n",
      "43 20130131.csv 500\n",
      "44 20130228.csv 500\n",
      "45 20130329.csv 500\n",
      "46 20130426.csv 500\n",
      "47 20130531.csv 500\n",
      "48 20130628.csv 500\n",
      "49 20130731.csv 500\n",
      "50 20130830.csv 500\n",
      "51 20130930.csv 500\n",
      "52 20131031.csv 500\n",
      "53 20131129.csv 500\n",
      "54 20131231.csv 500\n",
      "55 20140130.csv 500\n",
      "56 20140228.csv 500\n",
      "57 20140331.csv 500\n",
      "58 20140430.csv 500\n",
      "59 20140530.csv 500\n",
      "60 20140630.csv 500\n",
      "61 20140731.csv 500\n",
      "62 20140829.csv 500\n",
      "63 20140930.csv 500\n",
      "64 20141031.csv 500\n",
      "65 20141128.csv 500\n",
      "66 20141231.csv 500\n",
      "67 20150130.csv 500\n",
      "68 20150227.csv 500\n",
      "69 20150331.csv 500\n",
      "70 20150430.csv 500\n",
      "71 20150529.csv 500\n",
      "72 20150630.csv 500\n",
      "73 20150731.csv 500\n",
      "74 20150831.csv 500\n",
      "75 20150930.csv 500\n",
      "76 20151030.csv 500\n",
      "77 20151130.csv 500\n",
      "78 20151231.csv 500\n",
      "79 20160129.csv 500\n",
      "80 20160229.csv 500\n",
      "81 20160331.csv 500\n",
      "82 20160429.csv 500\n",
      "83 20160531.csv 500\n",
      "84 20160630.csv 500\n",
      "85 20160729.csv 500\n",
      "86 20160831.csv 500\n",
      "87 20160930.csv 500\n",
      "88 20161031.csv 500\n",
      "89 20161130.csv 500\n",
      "90 20161230.csv 500\n",
      "91 20170119.csv 499\n",
      "92 20170228.csv 500\n",
      "93 20170331.csv 500\n",
      "94 20170428.csv 500\n",
      "95 20170531.csv 500\n",
      "96 20170630.csv 500\n",
      "97 20170731.csv 500\n",
      "98 20170816.csv 500\n",
      "99 20170831.csv 500\n",
      "100 20170929.csv 500\n",
      "101 20171031.csv 500\n",
      "102 20171130.csv 500\n",
      "103 20171229.csv 500\n"
     ]
    }
   ],
   "source": [
    "set_all = set()\n",
    "for i in range(0,len(time_line)):\n",
    "    temp_path = os.path.join(PATH,time_line[i])\n",
    "    df = pd.read_csv(temp_path, header = None, dtype=object)\n",
    "    print(i, time_line[i], len(df))\n",
    "    set_all = set(df[0]) | set_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_index = [0]\n",
    "temp = pd.read_csv(os.path.join(PATH,time_line[0]), header = None, dtype=object)\n",
    "for i in range(1,len(time_line)):\n",
    "    temp_path = os.path.join(PATH,time_line[i])\n",
    "    df = pd.read_csv(temp_path, header = None, dtype=object)\n",
    "    if len(set(df[0]) - set(temp[0])) != 0:\n",
    "        use_index.append(i)\n",
    "        temp = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_index_group = [(0, use_index[1])]\n",
    "for i in range(2,len(use_index)):\n",
    "    use_index_group.append((use_index[i-1], use_index[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql_date_time(tup):\n",
    "    sql_str = \" \"\n",
    "    t1 = time_line[tup[0]][:8]\n",
    "    t1 = t1[:4] + \"-\" + t1[4:6] + \"-\" + t1[6:8]\n",
    "    t2 = time_line[tup[1]][:8]\n",
    "    t2 = t2[:4] + \"-\" + t2[4:6] + \"-\" + t2[6:8]\n",
    "    sql_str = sql_str +  \"date_time >= '\" + t1 + \"' and \" + \"date_time < '\" + t2 + \"'\"\n",
    "    return sql_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_str_list = []\n",
    "for tup in use_index_group:\n",
    "    sql_load_str = \"select * from session_end_ticks where \"\n",
    "    sql_load_str = sql_load_str + generate_sql_date_time(tup)\n",
    "    sql_str_list.append(sql_load_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 40s, sys: 7.83 s, total: 2min 48s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s_str = \"select * from session_end_ticks where date_time >= '2010-01-29' and date_time <= '2017-12-31' \"\n",
    "df = pd.read_sql(s_str, con = engine)\n",
    "df[\"unique_symbol\"] = df[\"unique_symbol\"].apply(lambda x:x[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"year\"] = df[\"date_time\"].apply(lambda x:str(x)[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df[\"instrument_id\"] == 105060000016)].copy()\n",
    "df = df[~(df[\"instrument_id\"] == 105060000905)].copy()\n",
    "df = df[~(df[\"instrument_id\"] == 105060000016)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剔除50、300、500股指数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_str = \"select * from adjustment \"\n",
    "ad_df = pd.read_sql(s_str, con = engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_df.columns = [\"instrument_id\", \"merge_time\", \"adjustment\"]\n",
    "ad_df[\"merge_time\"] = ad_df[\"merge_time\"].apply(lambda x: str(x))\n",
    "df[\"merge_time\"] = df[\"date_time\"].apply(lambda x: str(x)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = ad_df.groupby(\"instrument_id\")[\"adjustment\"]\n",
    "ad_df[\"adjustment_cumsum\"] = gb.transform(lambda x: x.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad_df[ad_df[\"instrument_id\"] == 104070000001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.merge(df, ad_df, how=\"left\",on=['instrument_id','merge_time'])\n",
    "res  = res.sort_values(by=\"merge_time\",ascending= True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res[res[\"instrument_id\"] == 104070000001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = res.groupby(\"instrument_id\")[\"adjustment_cumsum\"]\n",
    "res[\"adjustment_cumsum\"] = gb.transform(lambda x: x.fillna(method='ffill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"adjustment_cumsum\"] = res[\"adjustment_cumsum\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res[res[\"instrument_id\"] == 104070000001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv(\"index500_v1_adjustment.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
